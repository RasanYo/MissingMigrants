{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\rasan\\AppData\\Local\\Temp\\ipykernel_16880\\1018048934.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df = pd.read_csv('..\\data\\Missing_Migrants_Global_Figures_allData.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region of Incident</th>\n",
       "      <th>Incident Date</th>\n",
       "      <th>Number of Dead</th>\n",
       "      <th>Minimum Estimated Number of Missing</th>\n",
       "      <th>Total Number of Dead and Missing</th>\n",
       "      <th>Number of Survivors</th>\n",
       "      <th>Number of Females</th>\n",
       "      <th>Number of Males</th>\n",
       "      <th>Number of Children</th>\n",
       "      <th>Country of Origin</th>\n",
       "      <th>Region of Origin</th>\n",
       "      <th>Cause of Death</th>\n",
       "      <th>Country of Incident</th>\n",
       "      <th>Migration Route</th>\n",
       "      <th>Location of Incident</th>\n",
       "      <th>URL</th>\n",
       "      <th>Source Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North America</td>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>Central America</td>\n",
       "      <td>Mixed or unknown</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US-Mexico border crossing</td>\n",
       "      <td>Pima Country Office of the Medical Examiner ju...</td>\n",
       "      <td>http://humaneborders.info/</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North America</td>\n",
       "      <td>2014-01-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Latin America / Caribbean (P)</td>\n",
       "      <td>Mixed or unknown</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US-Mexico border crossing</td>\n",
       "      <td>Pima Country Office of the Medical Examiner ju...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>North America</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Latin America / Caribbean (P)</td>\n",
       "      <td>Mixed or unknown</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US-Mexico border crossing</td>\n",
       "      <td>Pima Country Office of the Medical Examiner ju...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North America</td>\n",
       "      <td>2014-01-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Central America</td>\n",
       "      <td>Violence</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US-Mexico border crossing</td>\n",
       "      <td>near Douglas, Arizona, USA</td>\n",
       "      <td>http://bit.ly/1qfIw00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Europe</td>\n",
       "      <td>2014-01-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sudan</td>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>Harsh environmental conditions / lack of adequ...</td>\n",
       "      <td>Russian Federation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Border between Russia and Estonia</td>\n",
       "      <td>http://bit.ly/1rTFTjR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Region of Incident Incident Date  Number of Dead  \\\n",
       "0      North America    2014-01-06             1.0   \n",
       "1      North America    2014-01-12             1.0   \n",
       "2      North America    2014-01-14             1.0   \n",
       "3      North America    2014-01-16             1.0   \n",
       "4             Europe    2014-01-16             1.0   \n",
       "\n",
       "   Minimum Estimated Number of Missing  Total Number of Dead and Missing  \\\n",
       "0                                  NaN                                 1   \n",
       "1                                  NaN                                 1   \n",
       "2                                  NaN                                 1   \n",
       "3                                  NaN                                 1   \n",
       "4                                  0.0                                 1   \n",
       "\n",
       "   Number of Survivors  Number of Females  Number of Males  \\\n",
       "0                  NaN                NaN              1.0   \n",
       "1                  NaN                NaN              NaN   \n",
       "2                  NaN                NaN              NaN   \n",
       "3                  NaN                NaN              1.0   \n",
       "4                  2.0                NaN              1.0   \n",
       "\n",
       "   Number of Children Country of Origin               Region of Origin  \\\n",
       "0                 NaN         Guatemala                Central America   \n",
       "1                 NaN           Unknown  Latin America / Caribbean (P)   \n",
       "2                 NaN           Unknown  Latin America / Caribbean (P)   \n",
       "3                 NaN            Mexico                Central America   \n",
       "4                 NaN             Sudan                Northern Africa   \n",
       "\n",
       "                                      Cause of Death  \\\n",
       "0                                   Mixed or unknown   \n",
       "1                                   Mixed or unknown   \n",
       "2                                   Mixed or unknown   \n",
       "3                                           Violence   \n",
       "4  Harsh environmental conditions / lack of adequ...   \n",
       "\n",
       "        Country of Incident            Migration Route  \\\n",
       "0  United States of America  US-Mexico border crossing   \n",
       "1  United States of America  US-Mexico border crossing   \n",
       "2  United States of America  US-Mexico border crossing   \n",
       "3  United States of America  US-Mexico border crossing   \n",
       "4        Russian Federation                        NaN   \n",
       "\n",
       "                                Location of Incident  \\\n",
       "0  Pima Country Office of the Medical Examiner ju...   \n",
       "1  Pima Country Office of the Medical Examiner ju...   \n",
       "2  Pima Country Office of the Medical Examiner ju...   \n",
       "3                         near Douglas, Arizona, USA   \n",
       "4                  Border between Russia and Estonia   \n",
       "\n",
       "                          URL  Source Quality  \n",
       "0  http://humaneborders.info/               5  \n",
       "1                         NaN               5  \n",
       "2                         NaN               5  \n",
       "3       http://bit.ly/1qfIw00               5  \n",
       "4       http://bit.ly/1rTFTjR               1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('..\\data\\Missing_Migrants_Global_Figures_allData.csv')\n",
    "ignore_cols = [\n",
    "    \"Main ID\", \n",
    "    \"Incident ID\", \n",
    "    \"Incident Type\", \n",
    "    \"Incident Year\", \n",
    "    \"Month\", \n",
    "    \"Coordinates\", \n",
    "    \"UNSD Geographical Grouping\", \n",
    "    \"Information Source\"\n",
    "]\n",
    "df = df.drop(columns=ignore_cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../../news_scraping/src')\n",
    "from scraper import Scraper\n",
    "\n",
    "path = '../data/articles_dataset.csv'\n",
    "file_exists = os.path.isfile(path)\n",
    "\n",
    "article_scraper = Scraper()\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Reading Article {index+1}\")\n",
    "    article = article_scraper.get_article_by_url(row[\"URL\"])\n",
    "    if article:\n",
    "        df.at[index, 'Content'] = article.text  # Append content if scraping was successful\n",
    "        row_data = df.loc[[index]].copy()\n",
    "        row_data.to_csv(path, mode='a', header=False, index=False)\n",
    "        print(f\"Appended article {index+1}\")\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region of Incident</th>\n",
       "      <th>Incident Date</th>\n",
       "      <th>Number of Dead</th>\n",
       "      <th>Minimum Estimated Number of Missing</th>\n",
       "      <th>Total Number of Dead and Missing</th>\n",
       "      <th>Number of Survivors</th>\n",
       "      <th>Number of Females</th>\n",
       "      <th>Number of Males</th>\n",
       "      <th>Number of Children</th>\n",
       "      <th>Country of Origin</th>\n",
       "      <th>Region of Origin</th>\n",
       "      <th>Cause of Death</th>\n",
       "      <th>Country of Incident</th>\n",
       "      <th>Migration Route</th>\n",
       "      <th>Location of Incident</th>\n",
       "      <th>URL</th>\n",
       "      <th>Source Quality</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North America</td>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>Central America</td>\n",
       "      <td>Mixed or unknown</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US-Mexico border crossing</td>\n",
       "      <td>Pima Country Office of the Medical Examiner ju...</td>\n",
       "      <td>http://humaneborders.info/</td>\n",
       "      <td>5</td>\n",
       "      <td>This web site is the result of ongoing partner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North America</td>\n",
       "      <td>2014-01-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Central America</td>\n",
       "      <td>Violence</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US-Mexico border crossing</td>\n",
       "      <td>near Douglas, Arizona, USA</td>\n",
       "      <td>http://bit.ly/1qfIw00</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mediterranean</td>\n",
       "      <td>2014-01-19</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Afghanistan,Syrian Arab Republic</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Drowning</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Eastern Mediterranean</td>\n",
       "      <td>Waters near Greece while being towed back to T...</td>\n",
       "      <td>http://bit.ly/2aMCwfg</td>\n",
       "      <td>5</td>\n",
       "      <td>Nine child­ren and three women died when their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Europe</td>\n",
       "      <td>2014-01-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iran (Islamic Republic of)</td>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>Vehicle accident / death linked to hazardous t...</td>\n",
       "      <td>France</td>\n",
       "      <td>English Channel to the UK</td>\n",
       "      <td>France - Calais</td>\n",
       "      <td>http://bit.ly/1icTIF9</td>\n",
       "      <td>4</td>\n",
       "      <td>On the night of 30th January 2014 a 17 year ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Caribbean</td>\n",
       "      <td>2014-01-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Haiti</td>\n",
       "      <td>Caribbean</td>\n",
       "      <td>Drowning</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>Caribbean to US</td>\n",
       "      <td>Off the coast of Fort Lauderdale, Florida</td>\n",
       "      <td>http://bit.ly/1zU2LSq</td>\n",
       "      <td>1</td>\n",
       "      <td>FORT LAUDERDALE, Fla. – Authorities have calle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Region of Incident Incident Date  Number of Dead  \\\n",
       "0      North America    2014-01-06             1.0   \n",
       "1      North America    2014-01-16             1.0   \n",
       "2      Mediterranean    2014-01-19            12.0   \n",
       "3             Europe    2014-01-30             1.0   \n",
       "4          Caribbean    2014-01-30             1.0   \n",
       "\n",
       "   Minimum Estimated Number of Missing  Total Number of Dead and Missing  \\\n",
       "0                                  NaN                                 1   \n",
       "1                                  NaN                                 1   \n",
       "2                                  NaN                                12   \n",
       "3                                  0.0                                 1   \n",
       "4                                  NaN                                 1   \n",
       "\n",
       "   Number of Survivors  Number of Females  Number of Males  \\\n",
       "0                  NaN                NaN              1.0   \n",
       "1                  NaN                NaN              1.0   \n",
       "2                  NaN                9.0              NaN   \n",
       "3                  2.0                NaN              1.0   \n",
       "4                  NaN                NaN              1.0   \n",
       "\n",
       "   Number of Children                 Country of Origin Region of Origin  \\\n",
       "0                 NaN                         Guatemala  Central America   \n",
       "1                 NaN                            Mexico  Central America   \n",
       "2                 3.0  Afghanistan,Syrian Arab Republic          Unknown   \n",
       "3                 1.0        Iran (Islamic Republic of)    Southern Asia   \n",
       "4                 1.0                             Haiti        Caribbean   \n",
       "\n",
       "                                      Cause of Death  \\\n",
       "0                                   Mixed or unknown   \n",
       "1                                           Violence   \n",
       "2                                           Drowning   \n",
       "3  Vehicle accident / death linked to hazardous t...   \n",
       "4                                           Drowning   \n",
       "\n",
       "        Country of Incident            Migration Route  \\\n",
       "0  United States of America  US-Mexico border crossing   \n",
       "1  United States of America  US-Mexico border crossing   \n",
       "2                    Greece      Eastern Mediterranean   \n",
       "3                    France  English Channel to the UK   \n",
       "4  United States of America            Caribbean to US   \n",
       "\n",
       "                                Location of Incident  \\\n",
       "0  Pima Country Office of the Medical Examiner ju...   \n",
       "1                         near Douglas, Arizona, USA   \n",
       "2  Waters near Greece while being towed back to T...   \n",
       "3                                    France - Calais   \n",
       "4          Off the coast of Fort Lauderdale, Florida   \n",
       "\n",
       "                          URL  Source Quality  \\\n",
       "0  http://humaneborders.info/               5   \n",
       "1       http://bit.ly/1qfIw00               5   \n",
       "2       http://bit.ly/2aMCwfg               5   \n",
       "3       http://bit.ly/1icTIF9               4   \n",
       "4       http://bit.ly/1zU2LSq               1   \n",
       "\n",
       "                                             Content  \n",
       "0  This web site is the result of ongoing partner...  \n",
       "1                                                NaN  \n",
       "2  Nine child­ren and three women died when their...  \n",
       "3  On the night of 30th January 2014 a 17 year ol...  \n",
       "4  FORT LAUDERDALE, Fla. – Authorities have calle...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cols = df.columns.to_list()\n",
    "df_cols.append(\"Content\")\n",
    "\n",
    "df = pd.read_csv('../data/articles_dataset.csv', header=None)\n",
    "df.columns = df_cols\n",
    "df.to_csv('../data/articles_dataset2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total X: 2456 | Total y: 2456\n",
      "X_train shape: (1964,)\n",
      "X_test shape: (492,)\n",
      "y_train shape: (1964, 17)\n",
      "y_test shape: (492, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['North America', '2014-01-06', 1.0, nan, 1, nan, nan, 1.0, nan,\n",
       "       'Guatemala', 'Central America', 'Mixed or unknown',\n",
       "       'United States of America', 'US-Mexico border crossing',\n",
       "       'Pima Country Office of the Medical Examiner jurisdiction, Arizona, USA (see coordinates for exact location)',\n",
       "       'http://humaneborders.info/', 5], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = df.iloc[:, -1:].to_numpy()\n",
    "X = np.array([x[0] for x in X])\n",
    "y = df.iloc[:, :-1]\n",
    "label_names = y.columns.to_list()\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting splits\n",
    "print(f\"Total X: {len(X)} | Total y: {len(y)}\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.40.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        labels = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        labels += [0] * (self.max_len - len(labels))  # Pad labels to match encoding length\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "\n",
    "num_labels = len(label_names)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_texts, train_labels = (X_train, y_train)\n",
    "val_texts, val_labels = (X_test, y_test)\n",
    "\n",
    "\n",
    "train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_len=1024)\n",
    "val_dataset = NewsDataset(val_texts, val_labels, tokenizer, max_len=1024)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/123 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (17,) (1007,) (17,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      8\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rasan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\rasan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\rasan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\rasan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[42], line 29\u001b[0m, in \u001b[0;36mNewsDataset.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[item]\n\u001b[0;32m     18\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m     19\u001b[0m     text,\n\u001b[0;32m     20\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 29\u001b[0m \u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pad labels to match encoding length\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     34\u001b[0m }\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (17,) (1007,) (17,) "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Training loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
